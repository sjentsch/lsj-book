<?xml version="1.0" encoding=""utf-8"" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">  
<!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->  
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head><title>8 Estimating unknown quantities from a sample</title> 
<meta http-equiv="Content-Type" content="text/html; charset="utf-8"" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<!-- 2,html,xhtml,NoFonts,ext=htm,charset="utf-8" --> 
<meta name="src" content="lsj.tex" /> 
<link rel="stylesheet" type="text/css" href="lsj.css" /> 
</head><body 
>
   <!--l. 4--><div class="crosslinks"><p class="noindent">[<a 
href="lsjch9.html" >next</a>] [<a 
href="lsjch7.html" >prev</a>] [<a 
href="lsjch7.html#taillsjch7.html" >prev-tail</a>] [<a 
href="#taillsjch8.html">tail</a>] [<a 
href="lsjpa4.html#lsjch8.html" >up</a>] </p></div>
   <h2 class="chapterHead"><span class="titlemark">Chapter 8</span><br /><a 
 id="x58-1300008"></a>Estimating unknown quantities from a sample</h2>
<!--l. 7--><p class="noindent" >At the start of the last chapter I highlighted the critical distinction between descriptive statistics
and inferential statistics. As discussed in Chapter <a 
href="lsjch4.html#x27-670004">4<!--tex4ht:ref: ch:descriptives --></a>, the role of descriptive statistics is to concisely
summarise what we do know. In contrast, the purpose of inferential statistics is to “learn what we
do not know from what we do”. Now that we have a foundation in probability theory we are in a
good position to think about the problem of statistical inference. What kinds of things would we
like to learn about? And how do we learn them? These are the questions that lie at the heart of
inferential statistics, and they are traditionally divided into two “big ideas”: estimation and
hypothesis testing. The goal in this chapter is to introduce the ﬁrst of these big ideas, estimation
theory, but I’m going to witter on about sampling theory ﬁrst because estimation theory
doesn’t make sense until you understand sampling. As a consequence, this chapter divides
naturally into two parts Sections <a 
href="#x58-1310008.1">8.1<!--tex4ht:ref: sec:srs --></a> through <a 
href="#x58-1380008.3">8.3<!--tex4ht:ref: sec:samplesandclt --></a> are focused on sampling theory, and
Sections <a 
href="#x58-1420008.4">8.4<!--tex4ht:ref: sec:pointestimates --></a> and <a 
href="#x58-1450008.5">8.5<!--tex4ht:ref: sec:ci --></a> make use of sampling theory to discuss how statisticians think about
estimation.
</p>
   <h3 class="sectionHead"><span class="titlemark">8.1   </span> <a 
 id="x58-1310008.1"></a>Samples, populations and sampling </h3>
<!--l. 12--><p class="noindent" >In the prelude to Part IV I discussed the riddle of induction and highlighted the fact that all
learning requires you to make assumptions. Accepting that this is true, our ﬁrst task to come up
with some fairly general assumptions about data that make sense. This is where <span id="textcolor200">sampling
theory</span> comes in. If probability theory is the foundations upon which all statistical theory
builds, sampling theory is the frame around which you can build the rest of the house.
Sampling theory plays a huge role in specifying the assumptions upon which your statistical
inferences rely. And in order to talk about “making inferences” the way statisticians
think about it we need to be a bit more explicit about what it is that we’re drawing
inferences from (the sample) and what it is that we’re drawing inferences about (the
population).
</p><!--l. 14--><p class="indent" >   In almost every situation of interest what we have available to us as researchers is a <span id="textcolor201">sample</span> of
data. We might have run experiment with some number of participants, a polling company might
have phoned some number of people to ask questions about voting intentions, and so on. In this
way the data set available to us is ﬁnite and incomplete. We can’t possibly get every person in the
world to do our experiment, for example a polling company doesn’t have the time or the money
to ring up every voter in the country. In our earlier discussion of descriptive statistics
(Chapter <a 
href="lsjch4.html#x27-670004">4<!--tex4ht:ref: ch:descriptives --></a>) this sample was the only thing we were interested in. Our only goal was
to ﬁnd ways of describing, summarising and graphing that sample. This is about to
change.
                                                                                          

                                                                                          
</p><!--l. 16--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.1.1   </span> <a 
 id="x58-1320008.1.1"></a>Deﬁning a population </h4>
<!--l. 18--><p class="noindent" >A sample is a concrete thing. You can open up a data ﬁle and there’s the data from your sample. A
<span id="textcolor202">population</span>, on the other hand, is a more abstract idea. It refers to the set of all possible people,
or all possible observations, that you want to draw conclusions about and is generally much bigger
than the sample. In an ideal world the researcher would begin the study with a clear idea of
what the population of interest is, since the process of designing a study and testing
hypotheses with the data does depend on the population about which you want to make
statements.
</p><!--l. 20--><p class="indent" >   Sometimes it’s easy to state the population of interest. For instance, in the “polling company”
example that opened the chapter the population consisted of all voters enrolled at the time of the
study, millions of people. The sample was a set of 1000 people who all belong to that population. In
most studies the situation is much less straightforward. In a typical psychological experiment
determining the population of interest is a bit more complicated. Suppose I run an experiment
using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to
learn something about how the mind works. So, which of the following would count as “the
population”: </p>
      <ul class="itemize1">
      <li class="itemize">All of the undergraduate psychology students at the University of Adelaide?
      </li>
      <li class="itemize">Undergraduate psychology students in general, anywhere in the world?
      </li>
      <li class="itemize">Australians currently living?
      </li>
      <li class="itemize">Australians of similar ages to my sample?
      </li>
      <li class="itemize">Anyone currently alive?
      </li>
      <li class="itemize">Any human being, past, present or future?
      </li>
      <li class="itemize">Any biological organism with a suﬃcient degree of intelligence operating in a terrestrial
      environment?
      </li>
      <li class="itemize">Any intelligent being?</li></ul>
                                                                                          

                                                                                          
<!--l. 31--><p class="noindent" >Each of these deﬁnes a real group of mind-possessing entities, all of which might be of interest to me as
a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest.
As another example, consider the Wellesley-Croker game that we discussed in the prelude. The
sample here is a speciﬁc sequence of 12 wins and 0 losses for Wellesley. What is the population?
</p>
      <ul class="itemize1">
      <li class="itemize">All outcomes until Wellesley and Croker arrived at their destination?
      </li>
      <li class="itemize">All outcomes if Wellesley and Croker had played the game for the rest of their lives?
      </li>
      <li class="itemize">All outcomes if Wellseley and Croker lived forever and played the game until the world
      ran out of hills?
      </li>
      <li class="itemize">All outcomes if we created an inﬁnite set of parallel universes and the Wellesely/Croker
      pair made guesses about the same 12 hills in each universe?</li></ul>
<!--l. 38--><p class="noindent" >Again, it’s not obvious what the population is.
</p><!--l. 40--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.1.2   </span> <a 
 id="x58-1330008.1.2"></a>Simple random samples</h4>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1330011"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 43--><p class="noindent" >
</p><!--l. 44--><p class="noindent" ><img 
src="lsj56x.png" alt="PIC" class="graphics" width="250" height="144"  /><!--tex4ht:graphics  
name="lsj56x.png" src="../img/estimation/srs1.eps"  
--></p></div>
<br /> <div class="caption" 
><span class="id">Figure 8.1: </span><span  
class="content">Simple random sampling without replacement from a ﬁnite population</span></div><!--tex4ht:label?: x58-1330011 -->
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 51--><p class="indent" >   Irrespective of how I deﬁne the population, the critical point is that the sample is a subset of
the population and our goal is to use our knowledge of the sample to draw inferences about the
properties of the population. The relationship between the two depends on the procedure by which
the sample was selected. This procedure is referred to as a <span id="textcolor203">sampling method</span> and it is important
to understand why it matters.
</p><!--l. 53--><p class="indent" >   To keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a
unique letter printed on it so we can distinguish between the 10 chips. The chips come in
two colours, black and white. This set of chips is the population of interest and it is
depicted graphically on the left of Figure <a 
href="#x58-1330011">8.1<!--tex4ht:ref: fig:srs1 --></a>. As you can see from looking at the picture
there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know
that unless we looked in the bag. Now imagine you run the following “experiment”: you
shake up the bag, close your eyes, and pull out 4 chips without putting any of them
back into the bag. First out comes the a chip (black), then the c chip (white), then j
(white) and then ﬁnally b (black). If you wanted you could then put all the chips back in
the bag and repeat the experiment, as depicted on the right hand side of Figure <a 
href="#x58-1330011">8.1<!--tex4ht:ref: fig:srs1 --></a>.
Each time you get diﬀerent results but the procedure is identical in each case. The fact
that the same procedure can lead to diﬀerent results each time we refer to as a random
process.<span class="footnote-mark"><a 
href="lsj59.html#fn1x9"><sup class="textsuperscript">1</sup></a></span><a 
 id="x58-133002f1"></a> 
However, because we shook the bag before pulling any chips out, it seems reasonable to think that
every chip has the same chance of being selected. A procedure in which every member of the
population has the same chance of being selected is called a <span id="textcolor205">simple random sample</span>. The fact
that we did not put the chips back in the bag after pulling them out means that you can’t observe
the same thing twice, and in such cases the observations are said to have been sampled <span id="textcolor206">without
replacement</span>.
</p><!--l. 55--><p class="indent" >   To help make sure you understand the importance of the sampling procedure, consider an
alternative way in which the experiment could have been run. Suppose that my 5-year old son had
opened the bag and decided to pull out four black chips without putting any of them back
in the bag. This biased sampling scheme is depicted in Figure <a 
href="#x58-1330032">8.2<!--tex4ht:ref: fig:brs --></a>. Now consider the
evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the
sampling scheme, does it not? If you know that the sampling scheme is biased to select
only black chips then a sample that consists of only black chips doesn’t tell you very
much about the population! For this reason statisticians really like it when a data set
can be considered a simple random sample, because it makes the data analysis much
easier.
                                                                                          

                                                                                          
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1330032"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 58--><p class="noindent" >
</p><!--l. 59--><p class="noindent" ><img 
src="lsj57x.png" alt="PIC" class="graphics" width="250" height="147"  /><!--tex4ht:graphics  
name="lsj57x.png" src="../img/estimation/brs.eps"  
--></p></div>
<br /> <div class="caption" 
><span class="id">Figure 8.2: </span><span  
class="content">Biased sampling without replacement from a ﬁnite population</span></div><!--tex4ht:label?: x58-1330032 -->
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1330043"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 68--><p class="noindent" >
</p><!--l. 69--><p class="noindent" ><img 
src="lsj58x.png" alt="PIC" class="graphics" width="250" height="144"  /><!--tex4ht:graphics  
name="lsj58x.png" src="../img/estimation/srs2.eps"  
--></p></div>
<br /> <div class="caption" 
><span class="id">Figure 8.3: </span><span  
class="content">Simple random sampling with replacement from a ﬁnite population</span></div><!--tex4ht:label?: x58-1330043 -->
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 77--><p class="indent" >   A third procedure is worth mentioning. This time around we close our eyes, shake the bag, and
pull out a chip. This time, however, we record the observation and then put the chip back in the
bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure
until we have 4 chips. Data sets generated in this way are still simple random samples, but because
we put the chips back in the bag immediately after drawing them it is referred to as a
sample <span id="textcolor207">with replacement</span>. The diﬀerence between this situation and the ﬁrst one is that
it is possible to observe the same population member multiple times, as illustrated in
Figure <a 
href="#x58-1330043">8.3<!--tex4ht:ref: fig:srs2 --></a>.
</p><!--l. 79--><p class="indent" >   In my experience, most psychology experiments tend to be sampling without replacement,
because the same person is not allowed to participate in the experiment twice. However, most
statistical theory is based on the assumption that the data arise from a simple random
sample with replacement. In real life this very rarely matters. If the population of interest
is large (e.g., has more than 10 entities!) the diﬀerence between sampling with- and
without- replacement is too small to be concerned with. The diﬀerence between simple
random samples and biased samples, on the other hand, is not such an easy thing to
dismiss.
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.1.3   </span> <a 
 id="x58-1340008.1.3"></a>Most samples are not simple random samples</h4>
<!--l. 83--><p class="noindent" >As you can see from looking at the list of possible populations that I showed above, it is almost
impossible to obtain a simple random sample from most populations of interest. When I
run experiments I’d consider it a minor miracle if my participants turned out to be
a random sampling of the undergraduate psychology students at Adelaide university,
even though this is by far the narrowest population that I might want to generalise to.
A thorough discussion of other types of sampling schemes is beyond the scope of this
book, but to give you a sense of what’s out there I’ll list a few of the more important
ones.
</p>
      <ul class="itemize1">
      <li class="itemize">Stratiﬁed sampling. Suppose your population is (or can be) divided into several diﬀerent
      sub-populations, or strata. Perhaps you’re running a study at several diﬀerent sites,
      for example. Instead of trying to sample randomly from the population as a whole,
      you instead try to collect a separate random sample from each of the strata. Stratiﬁed
      sampling is sometimes easier to do than simple random sampling, especially when the
      population is already divided into the distinct strata. It can also be more eﬃcient than
                                                                                          

                                                                                          
      simple random sampling, especially when some of the sub-populations are rare. For
      instance, when studying schizophrenia it would be much better to divide the population
      into two<span class="footnote-mark"><a 
href="lsj60.html#fn2x9"><sup class="textsuperscript">2</sup></a></span><a 
 id="x58-134001f2"></a> 
      strata (schizophrenic and not-schizophrenic) and then sample an equal number of people
      from each group. If you selected people randomly you would get so few schizophrenic
      people in the sample that your study would be useless. This speciﬁc kind of of stratiﬁed
      sampling  is  referred  to  as  oversampling because  it  makes  a  deliberate  attempt  to
      over-represent rare groups.
      </li>
      <li class="itemize">Snowball  sampling  is  a  technique  that  is  especially  useful  when  sampling  from  a
      “hidden” or hard to access population and is especially common in social sciences. For
      instance, suppose the researchers want to conduct an opinion poll among transgender
      people. The research team might only have contact details for a few trans folks, so the
      survey starts by asking them to participate (stage 1). At the end of the survey the
      participants are asked to provide contact details for other people who might want to
      participate. In stage 2 those new contacts are surveyed. The process continues until the
      researchers have suﬃcient data. The big advantage to snowball sampling is that it gets
      you data in situations that might otherwise be impossible to get any. On the statistical
      side, the main disadvantage is that the sample is highly non-random, and non-random
      in ways that are diﬃcult to address. On the real life side, the disadvantage is that the
      procedure can be unethical if not handled well, because hidden populations are often
      hidden for a reason. I chose transgender people as an example here to highlight this
      issue. If you weren’t careful you might end up outing people who don’t want to be
      outed (very, very bad form), and even if you don’t make that mistake it can still be
      intrusive to use people’s social networks to study them. It’s certainly very hard to get
      people’s informed consent before contacting them, yet in many cases the simple act of
      contacting them and saying “hey we want to study you” can be hurtful. Social networks
      are complex things, and just because you can use them to get data doesn’t always mean
      you should.
      </li>
      <li class="itemize">Convenience sampling is more or less what it sounds like. The samples are chosen
      in a way that is convenient to the researcher, and not selected at random from the
      population of interest. Snowball sampling is one type of convenience sampling, but
      there are many others. A common example in psychology are studies that rely on
                                                                                          

                                                                                          
      undergraduate psychology students. These samples are generally non-random in two
      respects. First, reliance on undergraduate psychology students automatically means
      that your data are restricted to a single sub-population. Second, the students usually
      get to pick which studies they participate in, so the sample is a self selected subset of
      psychology students and not a randomly selected subset. In real life most studies are
      convenience samples of one form or another. This is sometimes a severe limitation, but
      not always.</li></ul>
<!--l. 91--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.1.4   </span> <a 
 id="x58-1350008.1.4"></a>How much does it matter if you don’t have a simple random sample?</h4>
<!--l. 93--><p class="noindent" >Okay, so real world data collection tends not to involve nice simple random samples. Does that
matter? A little thought should make it clear to you that it can matter if your data are not a
simple random sample. Just think about the diﬀerence between Figures <a 
href="#x58-1330011">8.1<!--tex4ht:ref: fig:srs1 --></a> and <a 
href="#x58-1330032">8.2<!--tex4ht:ref: fig:brs --></a>.
However, it’s not quite as bad as it sounds. Some types of biased samples are entirely
unproblematic. For instance, when using a stratiﬁed sampling technique you actually know
what the bias is because you created it deliberately, often to increase the eﬀectiveness
of your study, and there are statistical techniques that you can use to adjust for the
biases you’ve introduced (not covered in this book!). So in those situations it’s not a
problem.
</p><!--l. 95--><p class="indent" >   More generally though, it’s important to remember that random sampling is a means to an end,
and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can
assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the
wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the
sample to be randomly generated in every respect, we only need it to be random with
respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study
looking at working memory capacity. In study 1, I actually have the ability to sample
randomly from all human beings currently alive, with one exception: I can only sample
people born on a Monday. In study 2, I am able to sample randomly from the Australian
population. I want to generalise my results to the population of all living humans. Which
study is better? The answer, obviously, is study 1. Why? Because we have no reason
to think that being “born on a Monday” has any interesting relationship to working
memory capacity. In contrast, I can think of several reasons why “being Australian” might
matter. Australia is a wealthy, industrialised country with a very well-developed education
system. People growing up in that system will have had life experiences much more similar
to the experiences of the people who designed the tests for working memory capacity.
This shared experience might easily translate into similar beliefs about how to “take a
                                                                                          

                                                                                          
test”, a shared assumption about how psychological experimentation works, and so on.
These things might actually matter. For instance, “test taking” style might have taught
the Australian participants how to direct their attention exclusively on fairly abstract
test materials much more than people who haven’t grown up in a similar environment.
This could therefore lead to a misleading picture of what working memory capacity
is.
</p><!--l. 97--><p class="indent" >   There are two points hidden in this discussion. First, when designing your own studies, it’s
important to think about what population you care about and try hard to sample in a way that is
appropriate to that population. In practice, you’re usually forced to put up with a “sample of
convenience” (e.g., psychology lecturers sample psychology students because that’s the least
expensive way to collect data, and our coﬀers aren’t exactly overﬂowing with gold), but if so you
should at least spend some time thinking about what the dangers of this practice might be.
Second, if you’re going to criticise someone else’s study because they’ve used a sample of
convenience rather than laboriously sampling randomly from the entire human population, at
least have the courtesy to oﬀer a speciﬁc theory as to how this might have distorted the
results.
</p><!--l. 99--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.1.5   </span> <a 
 id="x58-1360008.1.5"></a>Population parameters and sample statistics</h4>
<!--l. 101--><p class="noindent" >Okay. Setting aside the thorny methodological issues associated with obtaining a random sample,
let’s consider a slightly diﬀerent issue. Up to this point we have been talking about
populations the way a scientist might. To a psychologist a population might be a group
of people. To an ecologist a population might be a group of bears. In most cases the
populations that scientists care about are concrete things that actually exist in the real world.
Statisticians, however, are a funny lot. On the one hand, they are interested in real
world data and real science in the same way that scientists are. On the other hand, they
also operate in the realm of pure abstraction in the way that mathematicians do. As a
consequence, statistical theory tends to be a bit abstract in how a population is deﬁned. In
much the same way that psychological researchers operationalise our abstract theoretical
ideas in terms of concrete measurements (Section <a 
href="lsjch2.html#x10-110002.1">2.1<!--tex4ht:ref: sec:measurement --></a>), statisticians operationalise the
concept of a “population” in terms of mathematical objects that they know how to work
with. You’ve already come across these objects in Chapter <a 
href="lsjch7.html#x53-1160007">7<!--tex4ht:ref: ch:probability --></a>. They’re called probability
distributions.
</p><!--l. 103--><p class="indent" >   The idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the
population of interest is a group of actual humans who have IQ scores. A statistician “simpliﬁes”
this by operationally deﬁning the population as the probability distribution depicted in
                                                                                          

                                                                                          
Figure <a 
href="#x58-1360014">8.4<!--tex4ht:ref: fig:IQdist --></a>a. IQ tests are designed so that the average IQ is 100, the standard deviation of IQ
scores is 15, and the distribution of IQ scores is normal. These values are referred to as the
<span id="textcolor209">population parameters</span> because they are characteristics of the entire population. That is,
we say that the population mean μ is 100 and the population standard deviation σ is
15.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1360014"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 106--><p class="noindent" >
</p>
<div class="tabular">
 <table id="TBL-29" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-29-1g"><col 
id="TBL-29-1" /><col 
id="TBL-29-2" /><col 
id="TBL-29-3" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-29-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-29-1-1"  
class="td11"><img 
src="lsj59x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj59x.png" src="../img/estimation/IQpopulation.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-29-1-2"  
class="td11"><img 
src="lsj60x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj60x.png" src="../img/estimation/IQsample100.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-29-1-3"  
class="td11"><img 
src="lsj61x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj61x.png" src="../img/estimation/IQsample10000.eps"  
--></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-29-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-29-2-1"  
class="td11">           (a)                  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-29-2-2"  
class="td11">           (b)                  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-29-2-3"  
class="td11">            (c)                  </td></tr></table>
</div></div>
<br /> <div class="caption" 
><span class="id">Figure 8.4: </span><span  
class="content">The population distribution of IQ scores (panel a) and two samples drawn
randomly from it. In panel b we have a sample of 100 observations, and panel c we have a
sample of 10,000 observations.</span></div><!--tex4ht:label?: x58-1360014 -->
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 120--><p class="indent" >   Now suppose I run an experiment. I select 100 people at random and administer an IQ test,
giving me a simple random sample from the population. My sample would consist of a collection of
numbers like this:
                                                                                          

                                                                                          
</p>
   <div class="verbatim" id="verbatim-26">
<span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> 106</span><span 
class="cmtt-12"> 101</span><span 
class="cmtt-12"> 98</span><span 
class="cmtt-12"> 80</span><span 
class="cmtt-12"> 74</span><span 
class="cmtt-12"> ...</span><span 
class="cmtt-12"> 107</span><span 
class="cmtt-12"> 72</span><span 
class="cmtt-12"> 100</span>
</div>
<!--l. 123--><p class="nopar" >Each of these IQ scores is sampled from a normal distribution with mean 100 and standard
deviation 15. So if I plot a histogram of the sample I get something like the one shown in
Figure <a 
href="#x58-1360014">8.4<!--tex4ht:ref: fig:IQdist --></a>b. As you can see, the histogram is roughly the right shape but it’s a very crude
approximation to the true population distribution shown in Figure <a 
href="#x58-1360014">8.4<!--tex4ht:ref: fig:IQdist --></a>a. When I calculate the
mean of my sample, I get a number that is fairly close to the population mean 100 but not
identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the
standard deviation of their IQ scores is 15.9. These <span id="textcolor210">sample statistics</span> are properties of my data
set, and although they are fairly similar to the true population values they are not the same. In
general, sample statistics are the things you can calculate from your data set and the
population parameters are the things you want to learn about. Later on in this chapter I’ll
talk about how you can estimate population parameters using your sample statistics
(Section <a 
href="#x58-1420008.4">8.4<!--tex4ht:ref: sec:pointestimates --></a>) and how to work out how conﬁdent you are in your estimates (Section <a 
href="#x58-1450008.5">8.5<!--tex4ht:ref: sec:ci --></a>) but
before we get to that there’s a few more ideas in sampling theory that you need to know
about.
</p>
   <h3 class="sectionHead"><span class="titlemark">8.2   </span> <a 
 id="x58-1370008.2"></a>The law of large numbers </h3>
<!--l. 129--><p class="noindent" >In the previous section I showed you the results of one ﬁctitious IQ experiment with a sample size
of N   100. The results were somewhat encouraging as the true population mean is 100 and the
sample mean of 98.5 is a pretty reasonable approximation to it. In many scientiﬁc studies that level
of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we
want our sample statistics to be much closer to the population parameters, what can we do about
it?
</p><!--l. 131--><p class="indent" >   The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this
time measuring the IQs of 10,000 people. We can simulate the results of this experiment using
jamovi. The IQsim.omv ﬁle is a jamovi data ﬁle. In this ﬁle I have generated 10,000 random
numbers sampled from a normal distribution for a population with mean = 100 and sd = 15.
This was done by computing a new variable using the   = NORM(100,15) function. A
histogram and density plot shows that this larger sample is a much better approximation to
the true population distribution than the smaller one. This is reﬂected in the sample
statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard
                                                                                          

                                                                                          
deviation is 14.90. These values are now very close to the true population. See Figure
<a 
href="#x58-1370015">8.5<!--tex4ht:ref: fig:iqsim --></a>
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1370015"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 134--><p class="noindent" >

</p><!--l. 135--><p class="noindent" ><img 
src="../img/estimation/IQsim.png" alt="PIC"  
 /></p></div>
<br /> <div class="caption" 
><span class="id">Figure 8.5: </span><span  
class="content">A random sample drawn from a normal distribution using jamovi</span></div><!--tex4ht:label?: x58-1370015 -->
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 142--><p class="indent" >   I feel a bit silly saying this, but the thing I want you to take away from this is that large
samples generally give you better information. I feel silly saying it because it’s so bloody obvious
that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob
Bernoulli, one of the founders of probability theory, formalised this idea back in 1713
he was kind of a jerk about it. Here’s how he described the fact that we all share this
intuition:
      </p><div class="quote">
      <!--l. 144--><p class="noindent" >For even the most stupid of men, by some instinct of nature, by himself and
      without any instruction (which is a remarkable thing), is convinced that the more
      observations have been made, the less danger there is of wandering from one’s
      goal (see Stigler 1986, p. 65)</p></div>
<!--l. 147--><p class="noindent" >Okay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is
correct. It really does feel obvious that more data will give you better answers. The question is,
why is this so? Not surprisingly, this intuition that we all share turns out to be correct,
and statisticians refer to it as the <span id="textcolor211">law of large numbers</span>. The law of large numbers is
a mathematical law that applies to many diﬀerent sample statistics but the simplest
way to think about it is as a law about averages. The sample mean is the most obvious
example of a statistic that relies on averaging (because that’s what the mean is... an
average), so let’s look at that. When applied to the sample mean what the law of large
numbers states is that as the sample gets larger, the sample mean tends to get closer to
the true population mean. Or, to say it a little bit more precisely, as the sample size
“approaches” inﬁnity (written as N   8), the sample mean approaches the population mean
(<span class="bar-css">X</span>   μ).<span class="footnote-mark"><a 
href="lsj61.html#fn3x9"><sup class="textsuperscript">3</sup></a></span><a 
 id="x58-137002f3"></a> 
</p><!--l. 149--><p class="indent" >   I don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the
most important tools for statistical theory. The law of large numbers is the thing we can
use to justify our belief that collecting more and more data will eventually lead us to
the truth. For any particular data set the sample statistics that we calculate from it
will be wrong, but the law of large numbers tells us that if we keep collecting more
data those sample statistics will tend to get closer and closer to the true population
                                                                                          

                                                                                          
parameters.
</p>
   <h3 class="sectionHead"><span class="titlemark">8.3   </span> <a 
 id="x58-1380008.3"></a>Sampling distributions and the central limit theorem </h3>
<!--l. 154--><p class="noindent" >The law of large numbers is a very powerful tool but it’s not going to be good enough to
answer all our questions. Among other things, all it gives us is a “long run guarantee”. In
the long run, if we were somehow able to collect an inﬁnite amount of data, then the
law of large numbers guarantees that our sample statistics will be correct. But as John
Maynard Keynes famously argued in economics, a long run guarantee is of little use in real
life.
      </p><div class="quote">
      <!--l. 156--><p class="noindent" >[The] long run is a misleading guide to current aﬀairs. In the long run we are
      all dead. Economists set themselves too easy, too useless a task, if in tempestuous
      seasons they can only tell us, that when the storm is long past, the ocean is ﬂat
      again. (Keynes 1923, p. 80)</p></div>
<!--l. 159--><p class="noindent" >As in economics, so too in psychology and statistics. It is not enough to know that we will eventually
arrive at the right answer when calculating the sample mean. Knowing that an inﬁnitely large
data set will tell me the exact value of the population mean is cold comfort when my
actual data set has a sample size of N   100. In real life, then, we must know something
about the behaviour of the sample mean when it is calculated from a more modest data
set!
</p><!--l. 161--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.3.1   </span> <a 
 id="x58-1390008.3.1"></a>Sampling distribution of the mean </h4>
<!--l. 163--><p class="noindent" >With this in mind, let’s abandon the idea that our studies will have sample sizes of 10,000 and
consider instead a very modest experiment indeed. This time around we’ll sample N   5 people
and measure their IQ scores. As before, I can simulate this experiment in jamovi = NORM(100,15)
function, but I only need 5 participant IDs this time, not 10,000. These are the ﬁve numbers that
jamovi generated:
                                                                                          

                                                                                          
</p>
   <div class="verbatim" id="verbatim-27">
<span 
class="cmtt-12">90</span><span 
class="cmtt-12"> 82</span><span 
class="cmtt-12"> 94</span><span 
class="cmtt-12"> 99</span><span 
class="cmtt-12"> 110</span>
</div>
<!--l. 167--><p class="nopar" >
</p><!--l. 169--><p class="indent" >   The mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less
accurate than the previous experiment. Now imagine that I decided to <span id="textcolor213">replicate</span> the experiment.
That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and
measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates
these ﬁve numbers:
                                                                                          

                                                                                          
</p>
   <div class="verbatim" id="verbatim-28">
<span 
class="cmtt-12">78</span><span 
class="cmtt-12"> 88</span><span 
class="cmtt-12"> 111</span><span 
class="cmtt-12"> 111</span><span 
class="cmtt-12"> 117</span>
</div>
<!--l. 173--><p class="nopar" >
</p><!--l. 175--><p class="indent" >   This time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I
obtain the results shown in Table <a 
href="#x58-1390011">8.1<!--tex4ht:ref: tab:replications --></a>, and as you can see the sample mean varies from one
replication to the next.
</p>
   <div class="table">
                                                                                          

                                                                                          
<!--l. 177--><p class="indent" >   <a 
 id="x58-1390011"></a></p><hr class="float" /><div class="float" 
>
                                                                                          

                                                                                          
 <div class="caption" 
><span class="id">Table 8.1: </span><span  
class="content">Ten replications of the IQ experiment, each with a sample size of N   5.</span></div><!--tex4ht:label?: x58-1390011 -->
<div class="tabular"> <table id="TBL-30" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-30-1g"><col 
id="TBL-30-1" /><col 
id="TBL-30-2" /><col 
id="TBL-30-3" /><col 
id="TBL-30-4" /><col 
id="TBL-30-5" /><col 
id="TBL-30-6" /><col 
id="TBL-30-7" /></colgroup><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-30-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-1-1"  
class="td11">                 </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-2"  
class="td11">Person 1</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-3"  
class="td11">Person 2</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-4"  
class="td11">Person 3</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-5"  
class="td11">Person 4</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-6"  
class="td11">Person 5</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-1-7"  
class="td11">Sample Mean</td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-30-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-2-1"  
class="td11">Replication 1  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-2"  
class="td11">     90</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-3"  
class="td11">     82</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-4"  
class="td11">     94</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-5"  
class="td11">     99</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-6"  
class="td11">    110</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-2-7"  
class="td11">        95.0</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-3-1"  
class="td11">Replication 2  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-2"  
class="td11">     78</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-3"  
class="td11">     88</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-4"  
class="td11">    111</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-5"  
class="td11">    111</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-6"  
class="td11">    117</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-3-7"  
class="td11">       101.0</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-4-1"  
class="td11">Replication 3  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-2"  
class="td11">    111</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-3"  
class="td11">    122</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-4"  
class="td11">     91</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-5"  
class="td11">     98</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-6"  
class="td11">     86</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-4-7"  
class="td11">       101.6</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-5-1"  
class="td11">Replication 4  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-2"  
class="td11">     98</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-3"  
class="td11">     96</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-4"  
class="td11">    119</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-5"  
class="td11">     99</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-6"  
class="td11">    107</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-5-7"  
class="td11">       103.8</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-6-1"  
class="td11">Replication 5  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-2"  
class="td11">    105</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-3"  
class="td11">    113</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-4"  
class="td11">    103</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-5"  
class="td11">    103</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-6"  
class="td11">     98</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-6-7"  
class="td11">       104.4</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-7-1"  
class="td11">Replication 6  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-2"  
class="td11">     81</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-3"  
class="td11">     89</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-4"  
class="td11">     93</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-5"  
class="td11">     85</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-6"  
class="td11">    114</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-7-7"  
class="td11">        92.4</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-8-1"  
class="td11">Replication 7  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-2"  
class="td11">    100</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-3"  
class="td11">     93</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-4"  
class="td11">    108</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-5"  
class="td11">     98</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-6"  
class="td11">    133</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-8-7"  
class="td11">       106.4</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-9-1"  
class="td11">Replication 8  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-2"  
class="td11">    107</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-3"  
class="td11">    100</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-4"  
class="td11">    105</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-5"  
class="td11">    117</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-6"  
class="td11">     85</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-9-7"  
class="td11">       102.8</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-10-1"  
class="td11">Replication 9  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-2"  
class="td11">     86</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-3"  
class="td11">    119</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-4"  
class="td11">    108</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-5"  
class="td11">     73</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-6"  
class="td11">    116</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-10-7"  
class="td11">       100.4</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-11-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-11-1"  
class="td11">Replication 10</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-2"  
class="td11">     95</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-3"  
class="td11">    126</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-4"  
class="td11">    112</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-5"  
class="td11">    120</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-6"  
class="td11">     76</td><td  style="white-space:nowrap; text-align:right;" id="TBL-30-11-7"  
class="td11">       105.8</td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-30-12-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-12-1"  
class="td11">            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-30-13-"><td  style="white-space:nowrap; text-align:left;" id="TBL-30-13-1"  
class="td11">            </td></tr></table></div>
                                                                                          

                                                                                          
   </div><hr class="endfloat" />
   </div>
<!--l. 201--><p class="indent" >   Now suppose that I decided to keep going in this fashion, replicating this “ﬁve IQ scores”
experiment over and over again. Every time I replicate the experiment I write down the sample
mean. Over time, I’d be amassing a new data set, in which every experiment generates a single data
point. The ﬁrst 10 observations from my data set are the sample means listed in Table <a 
href="#x58-1390011">8.1<!--tex4ht:ref: tab:replications --></a>, so my
data set starts out like this:
                                                                                          

                                                                                          
</p>
   <div class="verbatim" id="verbatim-29">
<span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> 95.0</span><span 
class="cmtt-12"> 101.0</span><span 
class="cmtt-12"> 101.6</span><span 
class="cmtt-12"> 103.8</span><span 
class="cmtt-12"> 104.4</span><span 
class="cmtt-12"> ...</span>
</div>
<!--l. 204--><p class="nopar" >What if I continued like this for 10,000 replications, and then drew a histogram. Well that’s exactly
what I did, and you can see the results in Figure <a 
href="#x58-1390026">8.6<!--tex4ht:ref: fig:sampdistmean --></a>. As this picture illustrates, the average of 5 IQ
scores is usually between 90 and 110. But more importantly, what it highlights is that if we
replicate an experiment over and over again, what we end up with is a distribution of sample
means! This distribution has a special name in statistics, it’s called the <span id="textcolor214">sampling distribution of
the mean</span>.
</p><!--l. 207--><p class="indent" >   Sampling distributions are another important theoretical idea in statistics, and they’re crucial
for understanding the behaviour of small samples. For instance, when I ran the very ﬁrst “ﬁve IQ
scores” experiment, the sample mean turned out to be 95. What the sampling distribution in
Figure <a 
href="#x58-1390026">8.6<!--tex4ht:ref: fig:sampdistmean --></a> tells us, though, is that the “ﬁve IQ scores” experiment is not very accurate. If I repeat
the experiment, the sampling distribution tells me that I can expect to see a sample mean
anywhere between 80 and 120.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1390026"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 210--><p class="noindent" >

</p><!--l. 211--><p class="noindent" ><img 
src="../img/estimation/samplingDist4.png" alt="PIC"  
 />
<br /> </p><div class="caption" 
><span class="id">Figure 8.6: </span><span  
class="content">The sampling distribution of the mean for the “ﬁve IQ scores experiment”. If
you sample 5 people at random and calculate their average IQ you’ll almost certainly get a
number between 80 and 120, even though there are quite a lot of individuals who have IQs
above 120 or below 80. For comparison, the black line plots the population distribution of
IQ scores.</span></div><!--tex4ht:label?: x58-1390026 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1390037"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 220--><p class="noindent" >

</p><!--l. 221--><p class="noindent" ><img 
src="../img/estimation/samplingDistMax.png" alt="PIC"  
 />
<br /> </p><div class="caption" 
><span class="id">Figure 8.7: </span><span  
class="content">The sampling distribution of the maximum for the “ﬁve IQ scores experiment”.
If you sample 5 people at random and select the one with the highest IQ score you’ll probably
see someone with an IQ between 100 and 140.</span></div><!--tex4ht:label?: x58-1390037 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
   <h4 class="subsectionHead"><span class="titlemark">8.3.2   </span> <a 
 id="x58-1400008.3.2"></a>Sampling distributions exist for any sample statistic!</h4>
<!--l. 231--><p class="noindent" >One thing to keep in mind when thinking about sampling distributions is that any sample statistic
you might care to calculate has a sampling distribution. For example, suppose that each time I
replicated the “ﬁve IQ scores” experiment I wrote down the largest IQ score in the experiment.
This would give me a data set that started out like this:
                                                                                          

                                                                                          
</p>
   <div class="verbatim" id="verbatim-30">
<span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> </span><span 
class="cmtt-12"> 110</span><span 
class="cmtt-12"> 117</span><span 
class="cmtt-12"> 122</span><span 
class="cmtt-12"> 119</span><span 
class="cmtt-12"> 113</span><span 
class="cmtt-12"> ...</span>
</div>
<!--l. 234--><p class="nopar" >Doing this over and over again would give me a very diﬀerent sampling distribution, namely the
sampling distribution of the maximum. The sampling distribution of the maximum of 5 IQ
scores is shown in Figure <a 
href="#x58-1390037">8.7<!--tex4ht:ref: fig:sampdistmax --></a>. Not surprisingly, if you pick 5 people at random and then
ﬁnd the person with the highest IQ score, they’re going to have an above average IQ.
Most of the time you’ll end up with someone whose IQ is measured in the 100 to 140
range.
</p><!--l. 237--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.3.3   </span> <a 
 id="x58-1410008.3.3"></a>The central limit theorem </h4>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1410018"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 240--><p class="noindent" >
</p>
<div class="tabular">
 <table id="TBL-31" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-31-1g"><col 
id="TBL-31-1" /><col 
id="TBL-31-2" /><col 
id="TBL-31-3" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-31-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-31-1-1"  
class="td11"><img 
src="lsj62x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj62x.png" src="../img/estimation/samplingDist1.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-31-1-2"  
class="td11"><img 
src="lsj63x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj63x.png" src="../img/estimation/samplingDist2.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-31-1-3"  
class="td11"><img 
src="lsj64x.png" alt="PIC" class="graphics" width="128" height="128"  /><!--tex4ht:graphics  
name="lsj64x.png" src="../img/estimation/samplingDist3.eps"  
--></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-31-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-31-2-1"  
class="td11">           (a)                 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-31-2-2"  
class="td11">           (b)                  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-31-2-3"  
class="td11">            (c)                  </td></tr></table>
</div>
<br /> <div class="caption" 
><span class="id">Figure 8.8: </span><span  
class="content">An illustration of the how sampling distribution of the mean depends on sample
size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed
within each of these data sets. The histograms in these plots show the distribution of these
means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from
a normal distribution with mean 100 and standard deviation 15, which is shown as the solid
black line. In panel a, each data set contained only a single observation, so the mean of each
sample is just one person’s IQ score. As a consequence, the sampling distribution of the mean
is of course identical to the population distribution of IQ scores. However, when we raise the
sample size to 2 the mean of any one sample tends to be closer to the population mean than a
one person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower
than the population distribution. By the time we raise the sample size to 10 (panel c), we
can see that the distribution of sample means tend to be fairly tightly clustered around the
true population mean.</span></div><!--tex4ht:label?: x58-1410018 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 253--><p class="indent" >   At this point I hope you have a pretty good sense of what sampling distributions are, and in
particular what the sampling distribution of the mean is. In this section I want to talk about how
the sampling distribution of the mean changes as a function of sample size. Intuitively, you already
know part of the answer. If you only have a few observations, the sample mean is likely to be
quite inaccurate. If you replicate a small experiment and recalculate the mean you’ll
get a very diﬀerent answer. In other words, the sampling distribution is quite wide. If
you replicate a large experiment and recalculate the sample mean you’ll probably get
the same answer you got last time, so the sampling distribution will be very narrow.
You can see this visually in Figure <a 
href="#x58-1410018">8.8<!--tex4ht:ref: fig:IQsamp --></a>, showing that the bigger the sample size, the
narrower the sampling distribution gets. We can quantify this eﬀect by calculating the
standard deviation of the sampling distribution, which is referred to as the <span id="textcolor215">standard
error</span>. The standard error of a statistic is often denoted SE, and since we’re usually
interested in the standard error of the sample mean, we often use the acronym SEM. As
you can see just by looking at the picture, as the sample size N increases, the SEM
decreases.
</p><!--l. 255--><p class="indent" >   Okay, so that’s one part of the story. However, there’s something I’ve been glossing over so far.
All my examples up to this point have been based on the “IQ scores” experiments, and because IQ
scores are roughly normally distributed I’ve assumed that the population distribution is normal.
What if it isn’t normal? What happens to the sampling distribution of the mean? The remarkable
thing is this, no matter what shape your population distribution is, as N increases the
sampling distribution of the mean starts to look more like a normal distribution. To give
you a sense of this I ran some simulations. To do this, I started with the “ramped”
distribution shown in the histogram in Figure <a 
href="#x58-1410029">8.9<!--tex4ht:ref: fig:cltdemo --></a>. As you can see by comparing the triangular
shaped histogram to the bell curve plotted by the black line, the population distribution
doesn’t look very much like a normal distribution at all. Next, I simulated the results of
a large number of experiments. In each experiment I took N   2 samples from this
distribution, and then calculated the sample mean. Figure <a 
href="#x58-1410029">8.9<!--tex4ht:ref: fig:cltdemo --></a>b plots the histogram of these
sample means (i.e., the sampling distribution of the mean for N   2). This time, the
histogram produces a X-shaped distribution. It’s still not normal, but it’s a lot closer to
the black line than the population distribution in Figure <a 
href="#x58-1410029">8.9<!--tex4ht:ref: fig:cltdemo --></a>a. When I increase the
sample size to N   4, the sampling distribution of the mean is very close to normal
(Figure <a 
href="#x58-1410029">8.9<!--tex4ht:ref: fig:cltdemo --></a>c), and by the time we reach a sample size of N   8 it’s almost perfectly normal.
In other words, as long as your sample size isn’t tiny, the sampling distribution of the
mean will be approximately normal no matter what your population distribution looks
like!
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-1410029"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 258--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-32" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-32-1g"><col 
id="TBL-32-1" /><col 
id="TBL-32-2" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-32-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-32-1-1"  
class="td11"><img 
src="lsj65x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj65x.png" src="../img/estimation/cltDemo1.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-32-1-2"  
class="td11"><img 
src="lsj66x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj66x.png" src="../img/estimation/cltDemo2.eps"  
--></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-32-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-32-2-1"  
class="td11">                  (a)                            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-32-2-2"  
class="td11">                 (b)                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-32-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-32-3-1"  
class="td11"><img 
src="lsj67x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj67x.png" src="../img/estimation/cltDemo4.eps"  
--> </td><td  style="white-space:nowrap; text-align:center;" id="TBL-32-3-2"  
class="td11"><img 
src="lsj68x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj68x.png" src="../img/estimation/cltDemo8.eps"  
--></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-32-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-32-4-1"  
class="td11">                  (c)                            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-32-4-2"  
class="td11">                 (d)                           </td></tr></table>
</div>
<br /> <div class="caption" 
><span class="id">Figure 8.9: </span><span  
class="content">A demonstration of the central limit theorem. In panel a, we have a non-normal
population  distribution,  and  panels  b-d  show  the  sampling  distribution  of  the  mean  for
samples of size 2,4 and 8 for data drawn from the distribution in panel a. As you can see, even
though the original population distribution is non-normal the sampling distribution of the
mean becomes pretty close to normal by the time you have a sample of even 4 observations.
</span></div><!--tex4ht:label?: x58-1410029 -->
                                                                                          

                                                                                          
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 273--><p class="indent" >   On the basis of these ﬁgures, it seems like we have evidence for all of the following claims about
the sampling distribution of the mean. </p>
      <ul class="itemize1">
      <li class="itemize">The mean of the sampling distribution is the same as the mean of the population
      </li>
      <li class="itemize">The standard deviation of the sampling distribution (i.e., the standard error) gets
      smaller as the sample size increases
      </li>
      <li class="itemize">The shape of the sampling distribution becomes normal as the sample size increases</li></ul>
<!--l. 279--><p class="noindent" >As it happens, not only are all of these statements true, there is a very famous theorem in statistics
that proves all three of them, known as the <span id="textcolor216">central limit theorem</span>. Among other things, the
central limit theorem tells us that if the population distribution has mean μ and standard deviation
σ, then the sampling distribution of the mean also has mean μ and the standard error of the mean
is
</p>
   <center class="math-display" >
<img 
src="lsj69x.png" alt="         σ
SEM      ?----
          N  " class="math-display"  /></center> Because
we divide the population standard deviation σ by the square root of the sample size N, the SEM gets
smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes
normal.<span class="footnote-mark"><a 
href="lsj62.html#fn4x9"><sup class="textsuperscript">4</sup></a></span><a 
 id="x58-141003f4"></a> 
                                                                                          

                                                                                          
<!--l. 285--><p class="indent" >   This result is useful for all sorts of things. It tells us why large experiments are more reliable
than small ones, and because it gives us an explicit formula for the standard error it tells us
how much more reliable a large experiment is. It tells us why the normal distribution
is, well, normal. In real experiments, many of the things that we want to measure are
actually averages of lots of diﬀerent quantities (e.g., arguably, “general” intelligence as
measured by IQ is an average of a large number of “speciﬁc” skills and abilities), and
when that happens, the averaged quantity should follow a normal distribution. Because
of this mathematical law, the normal distribution pops up over and over again in real
data.
</p>
   <h3 class="sectionHead"><span class="titlemark">8.4   </span> <a 
 id="x58-1420008.4"></a>Estimating population parameters </h3>
<!--l. 290--><p class="noindent" >In all the IQ examples in the previous sections we actually knew the population parameters ahead
of time. As every undergraduate gets taught in their very ﬁrst lecture on the measurement of
intelligence, IQ scores are deﬁned to have mean 100 and standard deviation 15. However, this is a
bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know
this because the people who designed the tests have administered them to very large samples, and
have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of
course, it’s an important part of designing a psychological measurement. However, it’s important to
keep in mind that this theoretical mean of 100 only attaches to the population that the test
designers used to design the tests. Good test designers will actually go to some lengths to provide
“test norms” that can apply to lots of diﬀerent populations (e.g., diﬀerent age groups, nationalities
etc).
</p><!--l. 292--><p class="indent" >   This is very handy, but of course almost every research project of interest involves
looking at a diﬀerent population of people to those used in the test norms. For instance,
suppose you wanted to measure the eﬀect of low level lead poisoning on cognitive
functioning in Port Pirie, a South Australian industrial town with a lead smelter.
Perhaps you decide that you want to compare IQ scores among people in Port Pirie
to a comparable sample in Whyalla, a South Australian industrial town with a steel
                                                                                          

                                                                                          
reﬁnery.<span class="footnote-mark"><a 
href="lsj63.html#fn5x9"><sup class="textsuperscript">5</sup></a></span><a 
 id="x58-142001f5"></a> 
Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume
that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming
data that can automatically be applied to South Australian industrial towns. We’re going to
have to <span id="textcolor219">estimate</span> the population parameters from a sample of data. So how do we do
this?
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.4.1   </span> <a 
 id="x58-1430008.4.1"></a>Estimating the population mean</h4>
<!--l. 296--><p class="noindent" >Suppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The
average IQ score among these people turns out to be <span class="bar-css">X</span>   98.5. So what is the true mean IQ
for the entire population of Port Pirie? Obviously, we don’t know the answer to that
question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we
cannot give a deﬁnitive answer. Nevertheless, if I was forced at gunpoint to give a “best
guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best
guess.
</p><!--l. 298--><p class="indent" >   In this example estimating the unknown poulation parameter is straightforward. I calculate the
sample mean and I use that as my <span id="textcolor220">estimate of the population mean</span>. It’s pretty simple,
and in the next section I’ll explain the statistical justiﬁcation for this intuitive answer.
However, for the moment what I want to do is make sure you recognise that the sample
statistic and the estimate of the population parameter are conceptually diﬀerent things. A
sample statistic is a description of your data, whereas the estimate is a guess about the
population. With that in mind, statisticians often diﬀerent notation to refer to them. For
                                                                                          

                                                                                          
instance, if the true population mean is denoted μ, then we would use <img 
src="lsj70x.png" alt="μˆ"  class="circ"  /> to refer to
our estimate of the population mean. In contrast, the sample mean is denoted <span class="bar-css">X</span> or
sometimes m. However, in simple random samples the estimate of the population mean is
identical to the sample mean. If I observe a sample mean of <span class="bar-css">X</span>   98.5 then my estimate of
the population mean is also <img 
src="lsj71x.png" alt="ˆμ"  class="circ"  />   98.5. To help keep the notation clear, here’s a handy
table:
</p>
<div class="center" 
>
<!--l. 301--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-33" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-33-1g"><col 
id="TBL-33-1" /><col 
id="TBL-33-2" /><col 
id="TBL-33-3" /></colgroup><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-33-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-33-1-1"  
class="td11">                                      Symbol                                                           </td><td  style="white-space:nowrap; text-align:left;" id="TBL-33-1-2"  
class="td11">What is it?                             </td><td  style="white-space:wrap; text-align:left;" id="TBL-33-1-3"  
class="td11"><!--l. 305--><p class="noindent" >Do we know what it is?                </p></td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-33-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-33-2-1"  
class="td11">                                                                                  </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-33-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-33-3-1"  
class="td11"></td></tr><tr><td colspan="3"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-033-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-3-1"  
class="td11"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-033-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-4-1"  
class="td11">                                                                                  </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-033-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-5-1"  
class="td11"> <span class="bar-css">X</span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-033-5-2"  
class="td11">Sample mean                          </td><td  style="white-space:wrap; text-align:left;" id="TBL-033-5-3"  
class="td11"><!--l. 307--><p class="noindent" >Yes, calculated from the raw data  </p></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-033-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-6-1"  
class="td11">                                        μ                                                      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-033-6-2"  
class="td11">True population mean              </td><td  style="white-space:wrap; text-align:left;" id="TBL-033-6-3"  
class="td11"><!--l. 308--><p class="noindent" >Almost never known for sure         </p></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-033-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-7-1"  
class="td11">                                        <img 
src="lsj72x.png" alt="ˆμ"  class="circ"  />                                                      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-033-7-2"  
class="td11">Estimate of the population mean</td><td  style="white-space:wrap; text-align:left;" id="TBL-033-7-3"  
class="td11"><!--l. 309--><p class="noindent" >Yes, identical to the sample mean in
simple random samples                </p></td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-033-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-033-8-1"  
class="td11">                                                                                  </td></tr></table>
</div></div>
<!--l. 314--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.4.2   </span> <a 
 id="x58-1440008.4.2"></a>Estimating the population standard deviation</h4>
<!--l. 316--><p class="noindent" >So far, estimation seems pretty simple, and you might be wondering why I forced you to read
through all that stuﬀ about sampling theory. In the case of the mean our estimate of the
population parameter (i.e. <img 
src="lsj73x.png" alt="ˆμ"  class="circ"  />) turned out to identical to the corresponding sample statistic (i.e. <span class="bar-css">X</span>).
However, that’s not always true. To see this, let’s have a think about how to construct an <span id="textcolor221">estimate
of the population standard deviation</span>, which we’ll denote <img 
src="lsj74x.png" alt="σˆ"  class="circ"  />. What shall we use as our estimate
in this case? Your ﬁrst thought might be that we could do the same thing we did when estimating
the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do,
but not quite.
</p><!--l. 318--><p class="indent" >   Here’s why. Suppose I have a sample that contains a single observation. For this example, it
helps to consider a sample where you have no intuitions at all about what the true population
values might be, so let’s use something completely ﬁctitious. Suppose the observation in question
measures the cromulence of my shoes. It turns out that my shoes have a cromulence of 20. So here’s
my sample: </p>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 319--><p class="noindent" >
</p><!--l. 320--><p class="noindent" >20</p></div>
<!--l. 322--><p class="noindent" >This is a perfectly legitimate sample, even if it does have a sample size of N   1. It has a sample mean
of 20 and because every observation in this sample is equal to the sample mean (obviously!) it has a
sample standard deviation of 0. As a description of the sample this seems quite right, the sample
contains a single observation and therefore there is no variation observed within the sample. A
sample standard deviation of s   0 is the right answer here. But as an estimate of the population
standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at
all about what “cromulence” is, but we know something about data. The only reason that we don’t
see any variability in the sample is that the sample is too small to display any variation! So, if
you have a sample size of N   1 it feels like the right answer is just to say “no idea at
all”.
</p><!--l. 324--><p class="indent" >   Notice that you don’t have the same intuition when it comes to the sample mean and the
population mean. If forced to make a best guess about the population mean it doesn’t feel
completely insane to guess that the population mean is 20. Sure, you probably wouldn’t feel very
conﬁdent in that guess because you have only the one observation to work with, but it’s still the
best guess you can make.
</p><!--l. 326--><p class="indent" >   Let’s extend this example a little. Suppose I now make a second observation. My data set now
has N   2 observations of the cromulence of shoes, and the complete sample now looks like this:
</p>
<div class="center" 
>
<!--l. 327--><p class="noindent" >
</p><!--l. 328--><p class="noindent" >20, 22</p></div>
<!--l. 330--><p class="noindent" >This time around, our sample is just large enough for us to be able to observe some variability: two
observations is the bare minimum number needed for any variability to be observed! For our new
data set, the sample mean is <span class="bar-css">X</span>   21, and the sample standard deviation is s   1. What intuitions
do we have about the population? Again, as far as the population mean goes, the best guess we can
possibly make is the sample mean. If forced to guess we’d probably guess that the population mean
cromulence is 21. What about the standard deviation? This is a little more complicated. The
sample standard deviation is only based on two observations, and if you’re at all like me
you probably have the intuition that, with only two observations we haven’t given the
population “enough of a chance” to reveal its true variability to us. It’s not just that we
suspect that the estimate is wrong, after all with only two observations we expect it to be
wrong to some degree. The worry is that the error is systematic. Speciﬁcally, we suspect
                                                                                          

                                                                                          
that the sample standard deviation is likely to be smaller than the population standard
deviation.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-14400110"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 333--><p class="noindent" >
</p><!--l. 334--><p class="noindent" ><img 
src="lsj75x.png" alt="PIC" class="graphics" width="341" height="341"  /><!--tex4ht:graphics  
name="lsj75x.png" src="../img/estimation/samplingDistSampleSD.eps"  
-->
<br /> </p><div class="caption" 
><span class="id">Figure 8.10: </span><span  
class="content">The sampling distribution of the sample standard deviation for a “two IQ
scores” experiment. The true population standard deviation is 15 (dashed line), but as you
can see from the histogram the vast majority of experiments will produce a much smaller
sample standard deviation than this. On average, this experiment would produce a sample
standard deviation of only 8.5, well below the true value! In other words, the sample standard
deviation is a biased estimate of the population standard deviation.</span></div><!--tex4ht:label?: x58-14400110 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 341--><p class="indent" >   This intuition feels right, but it would be nice to demonstrate this somehow. There are
in fact mathematical proofs that conﬁrm this intuition, but unless you have the right
mathematical background they don’t help very much. Instead, what I’ll do is simulate the results
of some experiments. With that in mind, let’s return to our IQ studies. Suppose the
true population mean IQ is 100 and the standard deviation is 15. First I’ll conduct an
experiment in which I measure N   2 IQ scores and I’ll calculate the sample standard
deviation. If I do this over and over again, and plot a histogram of these sample standard
deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this
distribution in Figure <a 
href="#x58-14400110">8.10<!--tex4ht:ref: fig:sampdistsd --></a>. Even though the true population standard deviation is 15 the
average of the sample standard deviations is only 8.5. Notice that this is a very diﬀerent
result to what we found in Figure <a 
href="#x58-1410018">8.8<!--tex4ht:ref: fig:IQsamp --></a>b when we plotted the sampling distribution of the
mean, where the population mean is 100 and the average of the sample means is also
100.
</p><!--l. 343--><p class="indent" >   Now let’s extend the simulation. Instead of restricting ourselves to the situation where N   2,
let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and
average sample standard deviation as a function of sample size, you get the results shown in
Figure <a 
href="#x58-14400311">8.11<!--tex4ht:ref: fig:estimatorbias --></a>. On the left hand side (panel a) I’ve plotted the average sample mean and on the right
hand side (panel b) I’ve plotted the average standard deviation. The two plots are quite diﬀerent:on
average, the average sample mean is equal to the population mean. It is an <span id="textcolor222">unbiased estimator</span>,
which is essentially the reason why your best estimate for the population mean is the sample
mean.<span class="footnote-mark"><a 
href="lsj64.html#fn6x9"><sup class="textsuperscript">6</sup></a></span><a 
 id="x58-144002f6"></a> 
The plot on the right is quite diﬀerent: on average, the sample standard deviation s is smaller than
the population standard deviation σ. It is a <span id="textcolor224">biased estimator</span>. In other words, if we want
to make a “best guess” <img 
src="lsj76x.png" alt="ˆσ"  class="circ"  /> about the value of the population standard deviation σ we
should make sure our guess is a little bit larger than the sample standard deviation
s.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-14400311"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 346--><p class="noindent" >
</p>
<div class="tabular">
 <table id="TBL-34" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-34-1g"><col 
id="TBL-34-1" /><col 
id="TBL-34-2" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-34-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-34-1-1"  
class="td11"><img 
src="lsj77x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj77x.png" src="../img/estimation/biasMean.eps"  
--></td><td  style="white-space:nowrap; text-align:center;" id="TBL-34-1-2"  
class="td11"><img 
src="lsj78x.png" alt="PIC" class="graphics" width="199" height="199"  /><!--tex4ht:graphics  
name="lsj78x.png" src="../img/estimation/biasSD.eps"  
--></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-34-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-34-2-1"  
class="td11">                  (a)                            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-34-2-2"  
class="td11">                 (b)                           </td></tr></table>
</div>
<br /> <div class="caption" 
><span class="id">Figure 8.11: </span><span  
class="content">An illustration of the fact that the sample mean is an unbiased estimator of the
population mean (panel a), but the sample standard deviation is a biased estimator of the
population standard deviation (panel b). For the ﬁgure I generated 10,000 simulated data
sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size
of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with
a true population mean of 100 and standard deviation 15. On average, the sample means turn
out to be 100, regardless of sample size (panel a). However, the sample standard deviations
turn out to be systematically too small (panel b), especially for small sample sizes.</span></div><!--tex4ht:label?: x58-14400311 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 360--><p class="indent" >   The ﬁx to this systematic bias turns out to be very simple. Here’s how it works. Before tackling
the standard deviation let’s look at the variance. If you recall from Section <a 
href="lsjch4.html#x27-750004.2">4.2<!--tex4ht:ref: sec:var --></a>, the sample
variance is deﬁned to be the average of the squared deviations from the sample mean. That
is:
</p>
   <center class="math-display" >
<img 
src="lsj79x.png" alt="        N
s2    1--  pX     X¯q2
     N i  1   i   " class="math-display"  /></center> The
sample variance s<sup>2</sup> is a biased estimator of the population variance σ<sup>2</sup>. But as it turns out, we
only need to make a tiny tweak to transform this into an unbiased estimator. All we
have to do is divide by N   1 rather than by N. If we do that, we obtain the following
formula:
   <center class="math-display" >
<img 
src="lsj80x.png" alt="            N
ˆσ2    --1---   pXi    X¯q2
     N     1 i  1   " class="math-display"  /></center> This
is an unbiased estimator of the population variance σ. Moreover, this ﬁnally answers the question
we raised in Section <a 
href="lsjch4.html#x27-750004.2">4.2<!--tex4ht:ref: sec:var --></a>. Why did jamovi give us slightly diﬀerent answers for variance? It’s
because jamovi calculates <img 
src="lsj81x.png" alt="ˆσ"  class="circ"  /><sup>2</sup> not s<sup>2</sup>, that’s why. A similar story applies for the standard deviation.
If we divide by N   1 rather than N our estimate of the population standard deviation
becomes:
                                                                                          

                                                                                          
   <center class="math-display" >
<img 
src="lsj82x.png" alt="    gf  --------------------
    f    1     N
ˆσ    e  ------   pXi    X¯q2
       N     1i  1   " class="math-display"  /></center> and
when we use jamovi’s built in standard deviation function, what it’s doing is calculating <img 
src="lsj83x.png" alt="ˆσ"  class="circ"  />, not
s.<span class="footnote-mark"><a 
href="lsj65.html#fn7x9"><sup class="textsuperscript">7</sup></a></span><a 
 id="x58-144004f7"></a> 
<!--l. 375--><p class="indent" >   One ﬁnal point. In practice, a lot of people tend to refer to <img 
src="lsj91x.png" alt="ˆσ"  class="circ"  /> (i.e., the formula where we divide
by N   1) as the sample standard deviation. Technically, this is incorrect. The sample standard
deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same
thing, either conceptually or numerically. One is a property of the sample, the other is an estimated
characteristic of the population. However, in almost every real life application what we actually
care about is the estimate of the population parameter, and so people always report <img 
src="lsj92x.png" alt="ˆσ"  class="circ"  />
rather than s. This is the right number to report, of course. It’s just that people tend to
get a little bit imprecise about terminology when they write it up, because “sample
standard deviation” is shorter than “estimated population standard deviation”. It’s no big
deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s
important to keep the two concepts separate. It’s never a good idea to confuse “known
properties of your sample” with “guesses about the population from which it came”. The
moment you start thinking that s and <img 
src="lsj93x.png" alt="ˆσ"  class="circ"  /> are the same thing, you start doing exactly
that.
</p><!--l. 377--><p class="indent" >   To ﬁnish this section oﬀ, here’s another couple of tables to help keep things clear.
</p>
<div class="center" 
>
                                                                                          

                                                                                          
<!--l. 379--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-35" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-35-1g"><col 
id="TBL-35-1" /><col 
id="TBL-35-2" /><col 
id="TBL-35-3" /></colgroup><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-35-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-35-1-1"  
class="td11">                                      Symbol                                                           </td><td  style="white-space:nowrap; text-align:left;" id="TBL-35-1-2"  
class="td11">What is it?                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-35-1-3"  
class="td11">Do we know what it is?             </td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-35-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-35-2-1"  
class="td11">                                                                                  </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-35-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-35-3-1"  
class="td11"></td></tr><tr><td colspan="3"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-035-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-3-1"  
class="td11"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-035-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-4-1"  
class="td11">                                                                                  </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-035-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-5-1"  
class="td11"> s</td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-5-2"  
class="td11">Sample standard deviation     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-5-3"  
class="td11">Yes, calculated from the raw data</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-035-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-6-1"  
class="td11">                                        σ                                                      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-6-2"  
class="td11">Population standard deviation</td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-6-3"  
class="td11">Almost never known for sure      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-035-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-7-1"  
class="td11">                                        <img 
src="lsj94x.png" alt="ˆσ"  class="circ"  />                                                      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-7-2"  
class="td11">Estimate of the population     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-7-3"  
class="td11">Yes, but not the same as the       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-035-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-8-1"  
class="td11">                                                                                                          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-8-2"  
class="td11">standard deviation               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-035-8-3"  
class="td11">sample standard deviation          </td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-035-9-"><td  style="white-space:nowrap; text-align:center;" id="TBL-035-9-1"  
class="td11">                                                                                  </td></tr></table>
</div></div>
<div class="center" 
>
<!--l. 393--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-36" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-36-1g"><col 
id="TBL-36-1" /><col 
id="TBL-36-2" /><col 
id="TBL-36-3" /></colgroup><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-36-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-36-1-1"  
class="td11">                                      Symbol                                                           </td><td  style="white-space:nowrap; text-align:left;" id="TBL-36-1-2"  
class="td11">What is it?                     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-36-1-3"  
class="td11">Do we know what it is?             </td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-36-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-36-2-1"  
class="td11">                                                                                  </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-36-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-36-3-1"  
class="td11"></td></tr><tr><td colspan="3"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-036-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-3-1"  
class="td11"></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-036-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-4-1"  
class="td11">                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-036-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-5-1"  
class="td11">                                        s<sup>2</sup>                                                                                  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-5-2"  
class="td11">Sample variance              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-5-3"  
class="td11">Yes, calculated from the raw data</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-036-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-6-1"  
class="td11">                                        σ<sup>2</sup>                                                                                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-6-2"  
class="td11">Population variance         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-6-3"  
class="td11">Almost never known for sure      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-036-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-7-1"  
class="td11">                                        <img 
src="lsj95x.png" alt="ˆσ"  class="circ"  /><sup>2</sup>                                                                                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-7-2"  
class="td11">Estimate of the population</td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-7-3"  
class="td11">Yes, but not the same as the       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-036-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-8-1"  
class="td11">                                                                                                          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-8-2"  
class="td11">variance                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-036-8-3"  
class="td11">sample variance                        </td>
</tr><tr 
class="hline"><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-036-9-"><td  style="white-space:nowrap; text-align:center;" id="TBL-036-9-1"  
class="td11">                                                                                  </td></tr></table>
</div></div>
   <h3 class="sectionHead"><span class="titlemark">8.5   </span> <a 
 id="x58-1450008.5"></a>Estimating a conﬁdence interval</h3>
<!--l. 410--><p class="noindent" >
      </p><div class="quote">
      <!--l. 411--><p class="noindent" >Statistics means never having to say you’re certain <br 
class="newline" />                   – Unknown origin<span class="footnote-mark"><a 
href="lsj66.html#fn8x9"><sup class="textsuperscript">8</sup></a></span><a 
 id="x58-145001f8"></a> </p></div>
                                                                                          

                                                                                          
<!--l. 415--><p class="noindent" >Up to this point in this chapter, I’ve outlined the basics of sampling theory which statisticians rely
on to make guesses about population parameters on the basis of a sample of data. As this
discussion illustrates, one of the reasons we need all this sampling theory is that every data set
leaves us with a some of uncertainty, so our estimates are never going to be perfectly accurate. The
thing that has been missing from this discussion is an attempt to quantify the amount of
uncertainty that attaches to our estimate. It’s not enough to be able guess that, say, the mean IQ
of undergraduate psychology students is 115 (yes, I just made that number up). We also want to be
able to say something that expresses the degree of certainty that we have in our guess. For
example, it would be nice to be able to say that there is a 95% chance that the true
mean lies between 109 and 121. The name for this is a <span id="textcolor227">conﬁdence interval</span> for the
mean.
</p><!--l. 418--><p class="indent" >   Armed with an understanding of sampling distributions, constructing a conﬁdence interval for
the mean is actually pretty easy. Here’s how it works. Suppose the true population mean is μ and
the standard deviation is σ. I’ve just ﬁnished running my study that has N participants, and the
mean IQ among those participants is <span class="bar-css">X</span>. We know from our discussion of the central limit theorem
(Section <a 
href="#x58-1410008.3.3">8.3.3<!--tex4ht:ref: sec:clt --></a>) that the sampling distribution of the mean is approximately normal. We also know
from our discussion of the normal distribution Section <a 
href="lsjch7.html#x53-1260007.5">7.5<!--tex4ht:ref: sec:normal --></a> that there is a 95% chance that a
normally-distributed quantity will fall within about two standard deviations of the true
mean.
</p><!--l. 420--><p class="indent" >   To be more precise, the more correct answer is that there is a 95% chance that a
normally-distributed quantity will fall within 1.96 standard deviations of the true mean.
Next, recall that the standard deviation of the sampling distribution is referred to as the
standard error, and the standard error of the mean is written as SEM. When we put all
these pieces together, we learn that there is a 95% probability that the sample mean
<span class="bar-css">X</span> that we have actually observed lies within 1.96 standard errors of the population
mean.
</p><!--l. 424--><p class="indent" >   Mathematically, we write this as:
</p>
   <center class="math-display" >
<img 
src="lsj96x.png" alt="μ    p1.96    SEMq       ¯X      μ    p1.96    SEMq  " class="math-display"  /></center>
                                                                                          

                                                                                          
where the SEM is equal to σ{<img 
src="lsj97x.png" alt="?N---"  class="sqrt"  /> and we can be 95% conﬁdent that this is true. However, that’s
not answering the question that we’re actually interested in. The equation above tells us what we
should expect about the sample mean given that we know what the population parameters are.
What we want is to have this work the other way around. We want to know what we should believe
about the population parameters, given that we have observed a particular sample. However, it’s
not too diﬃcult to do this. Using a little high school algebra, a sneaky way to rewrite our equation
is like this:
   <center class="math-display" >
<img 
src="lsj98x.png" alt="¯X    p1.96    SEMq       μ      X¯    p1.96    SEMq  " class="math-display"  /></center>
What this is telling is is that the range of values has a 95% probability of containing the population
mean μ. We refer to this range as a <span id="textcolor228">95% conﬁdence interval</span>, denoted CI<sub>95</sub>. In short, as long as
N is suﬃciently large (large enough for us to believe that the sampling distribution
of the mean is normal), then we can write this as our formula for the 95% conﬁdence
interval:
   <center class="math-display" >
<img 
src="lsj99x.png" alt="
       ¯            -σ---
CI95    X      1.96    ? N
" class="math-display"  /></center>
<!--l. 438--><p class="indent" >   Of course, there’s nothing special about the number 1.96. It just happens to be the multiplier
you need to use if you want a 95% conﬁdence interval. If I’d wanted a 70% conﬁdence interval, I
would have used 1.04 as the magic number rather than 1.96.
                                                                                          

                                                                                          
</p><!--l. 440--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.5.1   </span> <a 
 id="x58-1460008.5.1"></a>A slight mistake in the formula</h4>
<!--l. 442--><p class="noindent" >As usual, I lied. The formula that I’ve given above for the 95% conﬁdence interval is approximately
correct, but I glossed over an important detail in the discussion. Notice my formula requires you to
use the standard error of the mean, SEM, which in turn requires you to use the true
population standard deviation σ. Yet, in Section <a 
href="#x58-1420008.4">8.4<!--tex4ht:ref: sec:pointestimates --></a> I stressed the fact that we don’t
actually know the true population parameters. Because we don’t know the true value
of σ we have to use an estimate of the population standard deviation <img 
src="lsj100x.png" alt="σˆ"  class="circ"  /> instead. This
is pretty straightforward to do, but this has the consequence that we need to use the
percentiles of the t-distribution rather than the normal distribution to calculate our
magic number, and the answer depends on the sample size. When N is very large, we get
pretty much the same value using the t-distribution or the normal distribution: 1.96.
But when N is small we get a much bigger number when we use the t distribution:
2.26.
</p><!--l. 444--><p class="indent" >   There’s nothing too mysterious about what’s happening here. Bigger values mean that the
conﬁdence interval is wider, indicating that we’re more uncertain about what the true value of μ
actually is. When we use the t distribution instead of the normal distribution we get bigger
numbers, indicating that we have more uncertainty. And why do we have that extra uncertainty?
Well, because our estimate of the population standard deviation <img 
src="lsj101x.png" alt="ˆσ"  class="circ"  /> might be wrong! If it’s wrong, it
implies that we’re a bit less sure about what our sampling distribution of the mean
actually looks like, and this uncertainty ends up getting reﬂected in a wider conﬁdence
interval.
</p><!--l. 446--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.5.2   </span> <a 
 id="x58-1470008.5.2"></a>Interpreting a conﬁdence interval</h4>
<!--l. 448--><p class="noindent" >The hardest thing about conﬁdence intervals is understanding what they mean. Whenever people
ﬁrst encounter conﬁdence intervals, the ﬁrst instinct is almost always to say that “there is a 95%
probability that the true mean lies inside the conﬁdence interval”. It’s simple and it seems to
capture the common sense idea of what it means to say that I am “95% conﬁdent”. Unfortunately,
it’s not quite right. The intuitive deﬁnition relies very heavily on your own personal beliefs about
the value of the population mean. I say that I am 95% conﬁdent because those are my beliefs. In
everyday life that’s perfectly okay, but if you remember back to Section <a 
href="lsjch7.html#x53-1180007.2">7.2<!--tex4ht:ref: sec:probmeaning --></a>, you’ll notice that
talking about personal belief and conﬁdence is a Bayesian idea. However, conﬁdence intervals are
not Bayesian tools. Like everything else in this chapter, conﬁdence intervals are frequentist tools,
and if you are going to use frequentist methods then it’s not appropriate to attach a
                                                                                          

                                                                                          
Bayesian interpretation to them. If you use frequentist methods, you must adopt frequentist
interpretations!
</p><!--l. 450--><p class="indent" >   Okay, so if that’s not the right answer, what is? Remember what we said about frequentist
probability. The only way we are allowed to make “probability statements” is to talk about a
sequence of events, and to count up the frequencies of diﬀerent kinds of events. From that
perspective, the interpretation of a 95% conﬁdence interval must have something to do with
replication. Speciﬁcally, if we replicated the experiment over and over again and computed a 95%
conﬁdence interval for each replication, then 95% of those intervals would contain the true mean.
More generally, 95% of all conﬁdence intervals constructed using this procedure should contain the
true population mean. This idea is illustrated in Figure <a 
href="#x58-14700112">8.12<!--tex4ht:ref: fig:cirep --></a>, which shows 50 conﬁdence intervals
constructed for a “measure 10 IQ scores” experiment (top panel) and another 50 conﬁdence
intervals for a “measure 25 IQ scores” experiment (bottom panel). A bit fortuitously, across the 100
replications that I simulated, it turned out that exactly 95 of them contained the true
mean.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                                          

                                                                                          
<a 
 id="x58-14700112"></a>
                                                                                          

                                                                                          
<div class="center" 
>
<!--l. 453--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-37" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-37-1g"><col 
id="TBL-37-1" /><col 
id="TBL-37-2" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-37-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-37-1-1"  
class="td11">(a)
    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-37-1-2"  
class="td11"><img 
src="lsj102x.png" alt="PIC" class="graphics" width="398" height="165"  /><!--tex4ht:graphics  
name="lsj102x.png" src="../img/estimation/confIntReplicated.eps"  
--></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-37-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-37-2-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-37-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-37-3-1"  
class="td11">   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-37-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-37-4-1"  
class="td11">(b)
     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-37-4-2"  
class="td11"><img 
src="lsj103x.png" alt="PIC" class="graphics" width="398" height="165"  /><!--tex4ht:graphics  
name="lsj103x.png" src="../img/estimation/confIntReplicated2.eps"  
--></td></tr></table></div>
<br /> <div class="caption" 
><span class="id">Figure 8.12: </span><span  
class="content">95% conﬁdence intervals. The top (panel a) shows 50 simulated replications of
an experiment in which we measure the IQs of 10 people. The dot marks the location of the
sample mean and the line shows the 95% conﬁdence interval. In total 47 of the 50 conﬁdence
intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks
do not. The lower graph (panel b) shows a similar simulation, but this time we simulate
replications of an experiment that measures the IQs of 25 people.</span></div><!--tex4ht:label?: x58-14700112 -->
</div>
                                                                                          

                                                                                          
   </div><hr class="endfigure" />
<!--l. 464--><p class="indent" >   The critical diﬀerence here is that the Bayesian claim makes a probability statement about the
population mean (i.e., it refers to our uncertainty about the population mean), which is not allowed
under the frequentist interpretation of probability because you can’t “replicate” a population!
In the frequentist claim, the population mean is ﬁxed and no probabilistic claims can
be made about it. Conﬁdence intervals, however, are repeatable so we can replicate
experiments. Therefore a frequentist is allowed to talk about the probability that the conﬁdence
interval (a random variable) contains the true mean, but is not allowed to talk about the
probability that the true population mean (not a repeatable event) falls within the conﬁdence
interval.
</p><!--l. 466--><p class="indent" >   I know that this seems a little pedantic, but it does matter. It matters because the diﬀerence in
interpretation leads to a diﬀerence in the mathematics. There is a Bayesian alternative to
conﬁdence intervals, known as credible intervals. In most situations credible intervals are quite
similar to conﬁdence intervals, but in other cases they are drastically diﬀerent. As promised,
though, I’ll talk more about the Bayesian perspective in Chapter <a 
href="lsjch16.html#x167-37000016">16<!--tex4ht:ref: ch:bayes --></a>.
</p>
   <h4 class="subsectionHead"><span class="titlemark">8.5.3   </span> <a 
 id="x58-1480008.5.3"></a>Calculating conﬁdence intervals in jamovi</h4>
<!--l. 471--><p class="noindent" >As far as I can tell, jamovi does not (yet) include a simple way to calculate conﬁdence intervals for
the mean as part of the ‘Descriptives’ functionality. But the ‘Descriptives’ do have a check box
for the S.E. Mean, so you can use this to calculate the lower 95% conﬁdence interval
as:
</p><!--l. 473--><p class="indent" >   Mean - (1.96 * S.E. Mean) , and the upper 95% conﬁdence interval as:
</p><!--l. 475--><p class="indent" >   Mean + (1.96 * S.E. Mean)
</p><!--l. 477--><p class="indent" >   95% conﬁdence intervals are the de facto standard in psychology. So, for example, if I load the
IQsim.omv ﬁle, check mean and S.E mean under ‘Descriptives, I can work out the conﬁdence
interval associated with the simulated mean IQ:
</p><!--l. 479--><p class="indent" >   Lower 95% CI = 99.68 - (1.96 * 0.15) = 99.39
</p><!--l. 481--><p class="indent" >   Upper 95% CI = 99.68 + (1.96 * 0.15) = 99.98
</p><!--l. 483--><p class="indent" >   So, in our simulated large sample data with N=10,000, the mean IQ score is 99.68 with a 95%
CI from 99.39 to 99.98. Hopefully that’s fairly clear. So, although there currently is not a
straightforward way to get jamovi to calculate the conﬁdence interval as part of the
variable ‘Descriptives’ options, if we wanted to we could pretty easily work it out by
hand.
</p><!--l. 485--><p class="indent" >   Similarly, when it comes to plotting conﬁdence intervals in jamovi, this is not (yet) available as
part of the ‘Descriptives’ options. However, when we get onto learning about speciﬁc statistical
tests, for example in Chapter <a 
href="lsjch13.html#x131-28000013">13<!--tex4ht:ref: ch:anova --></a>, we will see that we can plot conﬁdence intervals as
                                                                                          

                                                                                          
part of the data analysis. That’s pretty cool, so we’ll show you how to do that later
on.
</p><!--l. 488--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">8.6   </span> <a 
 id="x58-1490008.6"></a>Summary</h3>
<!--l. 490--><p class="noindent" >In this chapter I’ve covered two main topics. The ﬁrst half of the chapter talks about
sampling theory, and the second half talks about how we can use sampling theory to
construct estimates of the population parameters. The section breakdown looks like
this:
</p>
      <ul class="itemize1">
      <li class="itemize">Basic ideas about samples, sampling and populations (Section <a 
href="#x58-1310008.1">8.1<!--tex4ht:ref: sec:srs --></a>)
      </li>
      <li class="itemize">Statistical  theory  of  sampling:  the  law  of  large  numbers  (Section <a 
href="#x58-1370008.2">8.2<!--tex4ht:ref: sec:lawlargenumbers --></a>),  sampling
      distributions and the central limit theorem (Section <a 
href="#x58-1380008.3">8.3<!--tex4ht:ref: sec:samplesandclt --></a>).
      </li>
      <li class="itemize">Estimating means and standard deviations (Section <a 
href="#x58-1420008.4">8.4<!--tex4ht:ref: sec:pointestimates --></a>)
      </li>
      <li class="itemize">Estimating a conﬁdence interval (Section <a 
href="#x58-1450008.5">8.5<!--tex4ht:ref: sec:ci --></a>)</li></ul>
<!--l. 499--><p class="noindent" >As always, there’s a lot of topics related to sampling and estimation that aren’t covered in this
chapter, but for an introductory psychology class this is fairly comprehensive I think. For most
applied researchers you won’t need much more theory than this. One big question that I haven’t
touched on in this chapter is what you do when you don’t have a simple random sample. There is a
lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of
this book.
                                                                                          

                                                                                          
</p>
   <!--l. 5--><div class="crosslinks"><p class="noindent">[<a 
href="lsjch9.html" >next</a>] [<a 
href="lsjch7.html" >prev</a>] [<a 
href="lsjch7.html#taillsjch7.html" >prev-tail</a>] [<a 
href="lsjch8.html" >front</a>] [<a 
href="lsjpa4.html#lsjch8.html" >up</a>] </p></div>
<!--l. 5--><p class="indent" >   <a 
 id="taillsjch8.html"></a>  </p> 
</body></html> 
