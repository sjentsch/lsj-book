<?xml version="1.0" encoding=""utf-8"" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" 
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">  
<!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->  
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset="utf-8"" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<!-- 2,html,xhtml,NoFonts,ext=htm,charset="utf-8" --> 
<meta name="src" content="lsj.tex" /> 
<link rel="stylesheet" type="text/css" href="lsj.css" /> 
</head><body 
>
        <div class="footnote-text">
   <!--l. 316--><p class="indent" >     <span class="footnote-mark"><a 
 id="fn12x17">  <sup class="textsuperscript">12</sup></a></span><span id="textcolor475">In the interests of being completely honest, I should acknowledge that not all orthodox statistical tests rely
   on this silly assumption. There are a number of sequential analysis tools that are sometimes used in clinical trials
   and the like. These methods are built on the assumption that data are analysed as they arrive, and these tests aren’t
   horribly broken in the way I’m complaining about here. However, sequential analysis methods are constructed in a
   very diﬀerent fashion to the “standard” version of null hypothesis testing. They don’t make it into any introductory
   textbooks, and they’re not very widely used in the psychological literature. The concern I’m raising here is valid for
   every single orthodox test I’ve presented so far and for almost every test I’ve seen reported in the papers I
   read.</span></p></div>
       
</body></html> 
